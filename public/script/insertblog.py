import math
import time
import requests
from supabase import create_client, Client
from dotenv import load_dotenv
import os
import json

load_dotenv()

url: str = os.environ.get("SUPABASE_URL")
key: str = os.environ.get("SUPABASE_KEY")

# print(url)
# print(key)

supabase: Client = create_client(url, key)

JSON_FILE = "blogs_data.json"      # ä½ çš„ JSON æ–‡ä»¶å
TABLE = "blogs"  # è¡¨åæ¢æˆä½ çš„
BATCH_SIZE = 200  # æ¯æ¬¡å¤„ç† 200 æ¡ï¼Œå¯æ ¹æ®è´Ÿè½½è°ƒæ•´


def fetch_love_content(source_url):
    """ä½ çš„æƒ…ä¹¦æŠ“å–é€»è¾‘ï¼Œè¾“å…¥ä¸€æ¡ URLï¼Œè¿”å›æŠ“å–åˆ°çš„æ­£æ–‡æ–‡æœ¬"""
    try:
        resp = requests.get(source_url, timeout=10)
        resp.raise_for_status()
        # TODO: ä½ è‡ªå·±è§£ææ­£æ–‡
        return resp.text
    except Exception as e:
        print("æŠ“å–å¤±è´¥:", source_url, e)
        return None


def update_row(id, content):
    """æ›´æ–° Supabase çš„å†…å®¹å­—æ®µ"""
    return (
        supabase.table(TABLE)
        .update({"content": content, "is_valid": True})
        .eq("id", id)
        .execute()
    )


def process_batch(offset):
    """å¤„ç†ä¸€æ‰¹ 200 æ¡"""
    print(f"å¼€å§‹å¤„ç† offset={offset}")

    data = (
        supabase.table(TABLE)
        .select("*")
        .order("id")
        .range(offset, offset + BATCH_SIZE - 1)
        .execute()
    )

    rows = data.data
    if not rows:
        print("æ²¡æœ‰æ›´å¤šæ•°æ®")
        return False

    for row in rows:
        id = row["id"]
        source_url = row["source_url"]

        print(f"â†’ æŠ“å– ID={id} URL={source_url}")

        content = fetch_love_content(source_url)
        if content:
            update_row(id, content)
            print(f"âœ” æ›´æ–°æˆåŠŸ ID={id}")
        else:
            print(f"âœ˜ æŠ“å–å¤±è´¥ ID={id}")

        time.sleep(0.5)  # é˜²æ­¢çˆ¬è™«è¢«å°

    return True


def main():
    offset = 0
    while True:
        has_more = process_batch(offset)
        if not has_more:
            break
        offset += BATCH_SIZE
        time.sleep(2)  # é¿å… Supabase RLS æˆ–é™æµ

    print("å…¨éƒ¨å¤„ç†å®Œæˆ")


# if __name__ == "__main__":
#     main()


def count():

    # response = supabase.table(TABLE).select("*", count="exact").execute()
    # print(response.count)

    response = supabase.table(TABLE).select("*").limit(10).execute()

    print(response.data)

# count()


def insert_batch(batch, retry=3):
    """æ’å…¥ä¸€æ‰¹æ•°æ®åˆ° Supabaseï¼ˆå¸¦é‡è¯•ï¼‰"""
    for attempt in range(retry):
        try:
            response = supabase.table(TABLE).insert(batch).execute()
            print(f"âœ” æ’å…¥æˆåŠŸï¼š{len(batch)} æ¡,")
            print(response.data)
            return True
        except Exception as e:
            print(f"âŒ æ’å…¥å¤±è´¥ï¼Œç¬¬ {attempt+1} æ¬¡å°è¯•ï¼š", e)
            time.sleep(2)

    print("â›” å¤šæ¬¡å°è¯•ä»å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸€æ‰¹")
    return False

def test_insert_batch():

    with open(JSON_FILE,"r",encoding="utf-8") as f:
        data = json.load(f)

    total = len(data)
    # print(data)
    total_batches = math.ceil(total / BATCH_SIZE)

    print(f"æ€»å…± {total} æ¡è®°å½•ï¼Œå°†åˆ† {total_batches} æ‰¹æ’å…¥")

    for i in range(total_batches):
        start = i * BATCH_SIZE
        end = start + BATCH_SIZE
        batch = data[start:end]

        print(f"â†’ æ­£åœ¨æ’å…¥ç¬¬ {i+1}/{total_batches} æ‰¹ ({len(batch)} æ¡)")

        print(batch)
        success = insert_batch(batch)
    
        if not success:
            print(f"âš  ç¬¬ {i+1} æ‰¹æ’å…¥å¤±è´¥ï¼Œæš‚åœåç»§ç»­")
            time.sleep(3)
            continue  # ä¸å´©æºƒï¼Œç»§ç»­ä¸‹ä¸€æ‰¹

        time.sleep(1)  # é¿å… Supabase é™æµ

    print("ğŸ‰ å…¨éƒ¨æ’å…¥å®Œæˆ")

test_insert_batch()